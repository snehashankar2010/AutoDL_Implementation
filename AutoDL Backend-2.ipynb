{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8377e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37\\\\Lib\\\\site-packages')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "#import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score,accuracy_score\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n",
    "import keras\n",
    "# Conv1D + LSTM\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D,Conv2D,MaxPooling2D\n",
    "from keras.layers import GlobalMaxPooling1D, Flatten, Bidirectional, SpatialDropout1D, LSTM, Dense, Flatten\n",
    "\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications import EfficientNetB6\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a349665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c39bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Dataset Path for train:  \n",
      "jigsaw-toxic-comment-train.csv\n",
      "Enter the Dataset Path for test:  \n",
      "test.csv\n",
      "Enter the Dataset Path for valid:  \n",
      "validation.csv\n"
     ]
    }
   ],
   "source": [
    "train_path=input(\"Enter the Dataset Path for train:  \\n\")\n",
    "test_path=input(\"Enter the Dataset Path for test:  \\n\")\n",
    "valid_path=input(\"Enter the Dataset Path for valid:  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6153ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Type of dataset:\n",
      "1.Text\n",
      "2.CSV\n",
      "3.Excel\n",
      "4.Image\n",
      "5.Audio\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "inp=int(input(\"Enter the Type of dataset:\\n1.Text\\n2.CSV\\n3.Excel\\n4.Image\\n5.Audio\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa542fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if inp==1:\n",
    "    delim=input(\"Enter the delimiter present for given dataset:\")\n",
    "    df_train = pd.read_csv(train_path,delimiter=delim)\n",
    "    df_test = pd.read_csv(test_path,delimiter=delim)\n",
    "    df_valid = pd.read_csv(valid_path,delimiter=delim)\n",
    "elif inp==2:\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    df_valid = pd.read_csv(valid_path)\n",
    "elif inp==3:  \n",
    "    df_train = pd.read_excel(train_path)\n",
    "    df_test = pd.read_excel(test_path)\n",
    "    df_valid =pd.read_excel(valid_path)\n",
    "elif inp==4:\n",
    "    Image_Width = 512\n",
    "    Image_Height = 512\n",
    "    Image_Size = (Image_Width, Image_Height)\n",
    "    train_datagen = ImageDataGenerator(rotation_range=15,\n",
    "                                  rescale=1./255,\n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  width_shift_range=0.1,\n",
    "                                  height_shift_range=0.1,)\n",
    "    test_generator = test_datagen.flow_from_directory(path,\n",
    "                                                  target_size=Image_Size,\n",
    "                                                  batch_size = 32,\n",
    "                                                  class_mode='categorical')\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "elif inp==5:\n",
    "    print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bca87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01f84e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0\n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09b00ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12001, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.loc[:12000,:]\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66b4ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1403"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['comment_text'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7280d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train.comment_text.values, df_train.toxic.values, \n",
    "                                                  stratify=df_train.toxic.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48eccdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9600,), (2401,), (9600,), (2401,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd640a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "DLmodels = []\n",
    "DLhistory=[]\n",
    "AUCVal=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d49dee",
   "metadata": {},
   "source": [
    "# Deep Learning Model 1 -ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9f76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "batch_size=512\n",
    "epochs = 1000\n",
    "\n",
    "model_save = ModelCheckpoint('./JigsawToxic.h5', \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = True,\n",
    "                             monitor = 'val_loss', \n",
    "                             mode = 'min', verbose = 1)\n",
    "early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n",
    "                           patience = 50, mode = 'min', verbose = 1,\n",
    "                           restore_best_weights = True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.75, \n",
    "                              patience = 10, min_delta = 0.001, \n",
    "                              mode = 'min', verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e6edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 1500\n",
    "\n",
    "token.fit_on_texts(list(X_train) + list(X_test))\n",
    "xtrain_seq = token.texts_to_sequences(X_train)\n",
    "xvalid_seq = token.texts_to_sequences(X_test)\n",
    "\n",
    "#zero pad the sequences\n",
    "X_train_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4c4a38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def Create_Model_ANN(num_columns, num_labels,learning_rate):\n",
    "    model = Sequential()\n",
    "   \n",
    "    model.add(Dense(64,input_dim=num_columns, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_labels,activation='sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    #adam = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam,metrics=[keras.metrics.AUC(name='auc')])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = Create_Model_ANN(X_train_pad.shape[1], 1,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a7ab1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=X_train_pad.shape[1]\n",
    "\n",
    "model1 = keras.Sequential()\n",
    "\n",
    "w0=model.layers[0].get_weights()[0][:,:]\n",
    "b0=model.layers[0].get_weights()[1]\n",
    "\n",
    "model1.add(Dense(64,input_dim=input_size, activation='relu'))\n",
    "model1.layers[0].set_weights([w0, b0])\n",
    "\n",
    "# Add trained layers (with weight)\n",
    "for layer in model.layers[1:7]:\n",
    "    model1.add(layer)\n",
    "    \n",
    "# add output layer  sigmoid or softmax..    \n",
    "model1.add(Dense(1,activation='sigmoid'))\n",
    "#model_new.layers[0].set_weights([w0, b0])\n",
    "\n",
    "adam = Adam(lr=learning_rate)\n",
    "model1.compile(loss='binary_crossentropy', optimizer=adam,metrics=[keras.metrics.AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84a8bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 64)                96064     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 441,537\n",
      "Trainable params: 441,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c3d978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 21.1109 - auc: 0.5021\n",
      "Epoch 1: val_loss improved from inf to 2.15358, saving model to .\\JigsawToxic.h5\n",
      "19/19 [==============================] - 11s 188ms/step - loss: 20.3846 - auc: 0.5029 - val_loss: 2.1536 - val_auc: 0.5584 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 1.6975 - auc: 0.5340\n",
      "Epoch 2: val_loss improved from 2.15358 to 1.09560, saving model to .\\JigsawToxic.h5\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 1.6665 - auc: 0.5327 - val_loss: 1.0956 - val_auc: 0.5369 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6076 - auc: 0.5954\n",
      "Epoch 3: val_loss improved from 1.09560 to 0.70540, saving model to .\\JigsawToxic.h5\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 0.6036 - auc: 0.5960 - val_loss: 0.7054 - val_auc: 0.5594 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4185 - auc: 0.6619\n",
      "Epoch 4: val_loss improved from 0.70540 to 0.63392, saving model to .\\JigsawToxic.h5\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.4185 - auc: 0.6619 - val_loss: 0.6339 - val_auc: 0.5643 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.3459 - auc: 0.7155\n",
      "Epoch 5: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 62ms/step - loss: 0.3459 - auc: 0.7155 - val_loss: 0.6482 - val_auc: 0.5531 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.3007 - auc: 0.7662\n",
      "Epoch 6: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 59ms/step - loss: 0.3007 - auc: 0.7662 - val_loss: 0.6400 - val_auc: 0.5543 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.2696 - auc: 0.8071\n",
      "Epoch 7: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.2692 - auc: 0.8070 - val_loss: 0.6427 - val_auc: 0.5543 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2461 - auc: 0.8393\n",
      "Epoch 8: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.2461 - auc: 0.8393 - val_loss: 0.6457 - val_auc: 0.5569 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.2308 - auc: 0.8586\n",
      "Epoch 9: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.2308 - auc: 0.8586 - val_loss: 0.6840 - val_auc: 0.5527 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.2105 - auc: 0.8856\n",
      "Epoch 10: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 55ms/step - loss: 0.2115 - auc: 0.8847 - val_loss: 0.6753 - val_auc: 0.5567 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1985 - auc: 0.9008\n",
      "Epoch 11: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.1985 - auc: 0.9008 - val_loss: 0.6957 - val_auc: 0.5491 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1891 - auc: 0.9094\n",
      "Epoch 12: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 57ms/step - loss: 0.1891 - auc: 0.9094 - val_loss: 0.6860 - val_auc: 0.5593 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1787 - auc: 0.9219\n",
      "Epoch 13: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 61ms/step - loss: 0.1774 - auc: 0.9221 - val_loss: 0.7160 - val_auc: 0.5469 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1659 - auc: 0.9355\n",
      "Epoch 14: val_loss did not improve from 0.63392\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1660 - auc: 0.9348 - val_loss: 0.7368 - val_auc: 0.5614 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1573 - auc: 0.9420\n",
      "Epoch 15: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1573 - auc: 0.9420 - val_loss: 0.7034 - val_auc: 0.5561 - lr: 7.5000e-04\n",
      "Epoch 16/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1563 - auc: 0.9428\n",
      "Epoch 16: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 55ms/step - loss: 0.1555 - auc: 0.9429 - val_loss: 0.7379 - val_auc: 0.5484 - lr: 7.5000e-04\n",
      "Epoch 17/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1465 - auc: 0.9522\n",
      "Epoch 17: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 57ms/step - loss: 0.1466 - auc: 0.9521 - val_loss: 0.7394 - val_auc: 0.5597 - lr: 7.5000e-04\n",
      "Epoch 18/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1398 - auc: 0.9569\n",
      "Epoch 18: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 59ms/step - loss: 0.1399 - auc: 0.9563 - val_loss: 0.7977 - val_auc: 0.5538 - lr: 7.5000e-04\n",
      "Epoch 19/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1361 - auc: 0.9586\n",
      "Epoch 19: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 58ms/step - loss: 0.1357 - auc: 0.9586 - val_loss: 0.7621 - val_auc: 0.5584 - lr: 7.5000e-04\n",
      "Epoch 20/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1324 - auc: 0.9621\n",
      "Epoch 20: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 63ms/step - loss: 0.1319 - auc: 0.9622 - val_loss: 0.8325 - val_auc: 0.5528 - lr: 7.5000e-04\n",
      "Epoch 21/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1299 - auc: 0.9616\n",
      "Epoch 21: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 62ms/step - loss: 0.1299 - auc: 0.9616 - val_loss: 0.8425 - val_auc: 0.5495 - lr: 7.5000e-04\n",
      "Epoch 22/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1263 - auc: 0.9653\n",
      "Epoch 22: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 61ms/step - loss: 0.1263 - auc: 0.9653 - val_loss: 0.8183 - val_auc: 0.5523 - lr: 7.5000e-04\n",
      "Epoch 23/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1217 - auc: 0.9670\n",
      "Epoch 23: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 62ms/step - loss: 0.1217 - auc: 0.9670 - val_loss: 0.8994 - val_auc: 0.5567 - lr: 7.5000e-04\n",
      "Epoch 24/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1183 - auc: 0.9696\n",
      "Epoch 24: val_loss did not improve from 0.63392\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
      "19/19 [==============================] - 1s 49ms/step - loss: 0.1190 - auc: 0.9693 - val_loss: 0.9083 - val_auc: 0.5487 - lr: 7.5000e-04\n",
      "Epoch 25/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1145 - auc: 0.9707\n",
      "Epoch 25: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 44ms/step - loss: 0.1145 - auc: 0.9707 - val_loss: 0.9077 - val_auc: 0.5574 - lr: 5.6250e-04\n",
      "Epoch 26/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1159 - auc: 0.9711\n",
      "Epoch 26: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.1160 - auc: 0.9711 - val_loss: 0.9272 - val_auc: 0.5607 - lr: 5.6250e-04\n",
      "Epoch 27/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1127 - auc: 0.9714\n",
      "Epoch 27: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.1127 - auc: 0.9714 - val_loss: 0.9428 - val_auc: 0.5622 - lr: 5.6250e-04\n",
      "Epoch 28/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1091 - auc: 0.9749\n",
      "Epoch 28: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 51ms/step - loss: 0.1091 - auc: 0.9747 - val_loss: 1.0215 - val_auc: 0.5581 - lr: 5.6250e-04\n",
      "Epoch 29/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1100 - auc: 0.9712\n",
      "Epoch 29: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 65ms/step - loss: 0.1100 - auc: 0.9712 - val_loss: 0.9417 - val_auc: 0.5560 - lr: 5.6250e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1062 - auc: 0.9752\n",
      "Epoch 30: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.1070 - auc: 0.9748 - val_loss: 0.9588 - val_auc: 0.5641 - lr: 5.6250e-04\n",
      "Epoch 31/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1043 - auc: 0.9771\n",
      "Epoch 31: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.1043 - auc: 0.9771 - val_loss: 0.9625 - val_auc: 0.5586 - lr: 5.6250e-04\n",
      "Epoch 32/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1055 - auc: 0.9755\n",
      "Epoch 32: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 70ms/step - loss: 0.1055 - auc: 0.9755 - val_loss: 1.0277 - val_auc: 0.5487 - lr: 5.6250e-04\n",
      "Epoch 33/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0995 - auc: 0.9785\n",
      "Epoch 33: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.1002 - auc: 0.9781 - val_loss: 1.0996 - val_auc: 0.5522 - lr: 5.6250e-04\n",
      "Epoch 34/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.1002 - auc: 0.9783\n",
      "Epoch 34: val_loss did not improve from 0.63392\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
      "19/19 [==============================] - 1s 58ms/step - loss: 0.1001 - auc: 0.9787 - val_loss: 1.0882 - val_auc: 0.5547 - lr: 5.6250e-04\n",
      "Epoch 35/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0948 - auc: 0.9813\n",
      "Epoch 35: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.0948 - auc: 0.9813 - val_loss: 1.0424 - val_auc: 0.5552 - lr: 4.2187e-04\n",
      "Epoch 36/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0915 - auc: 0.9832\n",
      "Epoch 36: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.0914 - auc: 0.9832 - val_loss: 1.1192 - val_auc: 0.5471 - lr: 4.2187e-04\n",
      "Epoch 37/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0891 - auc: 0.9846\n",
      "Epoch 37: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 55ms/step - loss: 0.0891 - auc: 0.9846 - val_loss: 1.1497 - val_auc: 0.5558 - lr: 4.2187e-04\n",
      "Epoch 38/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0890 - auc: 0.9846\n",
      "Epoch 38: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 71ms/step - loss: 0.0884 - auc: 0.9847 - val_loss: 1.1459 - val_auc: 0.5574 - lr: 4.2187e-04\n",
      "Epoch 39/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0881 - auc: 0.9844\n",
      "Epoch 39: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 2s 129ms/step - loss: 0.0885 - auc: 0.9839 - val_loss: 1.1580 - val_auc: 0.5449 - lr: 4.2187e-04\n",
      "Epoch 40/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0865 - auc: 0.9857\n",
      "Epoch 40: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 73ms/step - loss: 0.0865 - auc: 0.9857 - val_loss: 1.2030 - val_auc: 0.5466 - lr: 4.2187e-04\n",
      "Epoch 41/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0842 - auc: 0.9862\n",
      "Epoch 41: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.0842 - auc: 0.9862 - val_loss: 1.2379 - val_auc: 0.5551 - lr: 4.2187e-04\n",
      "Epoch 42/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0834 - auc: 0.9871\n",
      "Epoch 42: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 2s 112ms/step - loss: 0.0834 - auc: 0.9871 - val_loss: 1.3004 - val_auc: 0.5397 - lr: 4.2187e-04\n",
      "Epoch 43/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0834 - auc: 0.9852\n",
      "Epoch 43: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.0834 - auc: 0.9852 - val_loss: 1.2585 - val_auc: 0.5483 - lr: 4.2187e-04\n",
      "Epoch 44/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0815 - auc: 0.9877\n",
      "Epoch 44: val_loss did not improve from 0.63392\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.0815 - auc: 0.9877 - val_loss: 1.2762 - val_auc: 0.5478 - lr: 4.2187e-04\n",
      "Epoch 45/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0781 - auc: 0.9889\n",
      "Epoch 45: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.0788 - auc: 0.9886 - val_loss: 1.3137 - val_auc: 0.5513 - lr: 3.1641e-04\n",
      "Epoch 46/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0788 - auc: 0.9888\n",
      "Epoch 46: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 55ms/step - loss: 0.0788 - auc: 0.9888 - val_loss: 1.3297 - val_auc: 0.5501 - lr: 3.1641e-04\n",
      "Epoch 47/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0778 - auc: 0.9890\n",
      "Epoch 47: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.0779 - auc: 0.9889 - val_loss: 1.3218 - val_auc: 0.5558 - lr: 3.1641e-04\n",
      "Epoch 48/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0763 - auc: 0.9896\n",
      "Epoch 48: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.0763 - auc: 0.9896 - val_loss: 1.3689 - val_auc: 0.5525 - lr: 3.1641e-04\n",
      "Epoch 49/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0776 - auc: 0.9877\n",
      "Epoch 49: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.0777 - auc: 0.9877 - val_loss: 1.3373 - val_auc: 0.5506 - lr: 3.1641e-04\n",
      "Epoch 50/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0824 - auc: 0.9877\n",
      "Epoch 50: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 59ms/step - loss: 0.0825 - auc: 0.9876 - val_loss: 1.3061 - val_auc: 0.5446 - lr: 3.1641e-04\n",
      "Epoch 51/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0804 - auc: 0.9870\n",
      "Epoch 51: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 64ms/step - loss: 0.0804 - auc: 0.9870 - val_loss: 1.3484 - val_auc: 0.5627 - lr: 3.1641e-04\n",
      "Epoch 52/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0749 - auc: 0.9898\n",
      "Epoch 52: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 2s 85ms/step - loss: 0.0743 - auc: 0.9901 - val_loss: 1.3787 - val_auc: 0.5598 - lr: 3.1641e-04\n",
      "Epoch 53/1000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.0731 - auc: 0.9903\n",
      "Epoch 53: val_loss did not improve from 0.63392\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.0731 - auc: 0.9903 - val_loss: 1.4480 - val_auc: 0.5444 - lr: 3.1641e-04\n",
      "Epoch 54/1000\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0733 - auc: 0.9903\n",
      "Epoch 54: val_loss did not improve from 0.63392\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.0728 - auc: 0.9906 - val_loss: 1.5173 - val_auc: 0.5543 - lr: 3.1641e-04\n",
      "Epoch 54: early stopping\n"
     ]
    }
   ],
   "source": [
    "history1=model1.fit(X_train_pad, y_train,validation_data=(xvalid_pad,y_test),callbacks = [model_save, early_stop, reduce_lr],\n",
    "          verbose=1,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743181d9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73263d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.572\n"
     ]
    }
   ],
   "source": [
    "y_pred = model1.predict(xvalid_pad)\n",
    "AUC1 = metrics.roc_auc_score(y_test,y_pred)\n",
    "print(\"AUC: {:.3f}\".format(AUC1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ef12ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://97bcb670-9830-487a-a45b-664fcdbc6cc9/assets\n"
     ]
    }
   ],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'ANN','AUC_Score': AUC1})\n",
    "model1.save('ANN2Model.h5')\n",
    "hist_df1 = pd.DataFrame(history1.history)\n",
    "hist_df1.to_csv('History1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a094a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "DLmodels.append(model1)\n",
    "DLhistory.append(history1)\n",
    "AUCVal.append(AUC1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b6ca5",
   "metadata": {},
   "source": [
    "# Deep Learning Model 2 -SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93369265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1500, 300)         13049100  \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 100)               40100     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,089,301\n",
      "Trainable params: 13,089,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
    "    model2 = Sequential()\n",
    "    model2.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     input_length=max_len))\n",
    "    model2.add(SimpleRNN(100))\n",
    "    \n",
    "    model2.add(Dense(1, activation='sigmoid'))\n",
    "    model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2ece43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 315s 2s/step - loss: 0.3074 - accuracy: 0.8996\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 204s 1s/step - loss: 0.1169 - accuracy: 0.9604\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 211s 1s/step - loss: 0.0148 - accuracy: 0.9956\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 213s 1s/step - loss: 0.0037 - accuracy: 0.9993\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 214s 1s/step - loss: 0.1636 - accuracy: 0.9377\n"
     ]
    }
   ],
   "source": [
    "history2=model2.fit(X_train_pad,\n",
    "          y_train, epochs=5, batch_size=64*strategy.num_replicas_in_sync) #Multiplying by Strategy to run on TPU's\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c090b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6616b4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.767\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model2.predict(xvalid_pad)\n",
    "AUC2 = metrics.roc_auc_score(y_test,y_pred2)\n",
    "print(\"AUC: {:.3f}\".format(AUC2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ad28754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b79b1f38-2ba9-46a8-8214-e4c4b0a150a7/assets\n"
     ]
    }
   ],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'SimpleRNN','AUC_Score': AUC2})\n",
    "model2.save('SimpleRNN2Model.h5')\n",
    "hist_df2 = pd.DataFrame(history2.history)\n",
    "hist_df2.to_csv('History2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d162015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DLmodels.append(model2)\n",
    "DLhistory.append(history2)\n",
    "AUCVal.append(AUC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866cef5a",
   "metadata": {},
   "source": [
    "## Deep Learning Model-3 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf2075c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [08:38, 4239.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24f50296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 43496/43496 [00:19<00:00, 2283.75it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52c15b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 1500, 300)         13049100  \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               160400    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,209,601\n",
      "Trainable params: 160,501\n",
      "Non-trainable params: 13,049,100\n",
      "_________________________________________________________________\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    \n",
    "    # A simple LSTM with glove embeddings and one dense layer\n",
    "    model3 = Sequential()\n",
    "    model3.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "\n",
    "    model3.add(LSTM(100,dropout=0.3, recurrent_dropout=0.3))\n",
    "    model3.add(Dense(1, activation='sigmoid'))\n",
    "    model3.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2429bfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 20138s 134s/step - loss: 0.2059 - accuracy: 0.9319\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 7317s 48s/step - loss: 0.1395 - accuracy: 0.9513\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 7040s 47s/step - loss: 0.1216 - accuracy: 0.9565\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 7102s 47s/step - loss: 0.1138 - accuracy: 0.9599\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 20984s 141s/step - loss: 0.1022 - accuracy: 0.9623\n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(X_train_pad,y_train, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e26af",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2c15162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.978\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = model3.predict(xvalid_pad)\n",
    "AUC3 = metrics.roc_auc_score(y_test,y_pred3)\n",
    "print(\"AUC: {:.3f}\".format(AUC3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4971777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://9d5d7c44-b93d-4853-a95e-67fcad68f81f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001DC04602CC8> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'LSTM','AUC_Score': AUC3})\n",
    "model3.save('LSTM2Model.h5')\n",
    "hist_df3 = pd.DataFrame(history3.history)\n",
    "hist_df3.to_csv('History3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3073ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "DLmodels.append(model3)\n",
    "DLhistory.append(history3)\n",
    "AUCVal.append(AUC3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f74d9b",
   "metadata": {},
   "source": [
    "## Deep Learning Model 4:GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a3439ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1500, 300)         13049100  \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 1500, 300)        0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 300)               540900    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,590,301\n",
      "Trainable params: 541,201\n",
      "Non-trainable params: 13,049,100\n",
      "_________________________________________________________________\n",
      "Wall time: 6.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # GRU with glove embeddings and two dense layers\n",
    "    model4 = Sequential()\n",
    "    model4.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "    model4.add(SpatialDropout1D(0.3))\n",
    "    model4.add(GRU(300))\n",
    "    model4.add(Dense(1, activation='sigmoid'))\n",
    "    model4.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])   \n",
    "    \n",
    "model4.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba55d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 34011s 228s/step - loss: 0.1980 - accuracy: 0.9334\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 31831s 212s/step - loss: 0.1281 - accuracy: 0.9518\n",
      "Epoch 3/5\n",
      "118/150 [======================>.......] - ETA: 2:46:38 - loss: 0.1032 - accuracy: 0.9620"
     ]
    }
   ],
   "source": [
    "history4=model4.fit(X_train_pad, y_train, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773456ca",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33618f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = model4.predict(xvalid_pad)\n",
    "AUC4 = metrics.roc_auc_score(y_test,y_pred4)\n",
    "print(\"AUC: {:.3f}\".format(AUC4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b91af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'GRU','AUC_Score': AUC4})\n",
    "model3.save('GRU2Model.h5')\n",
    "hist_df3 = pd.DataFrame(history4.history)\n",
    "hist_df3.to_csv('History4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "DLmodels.append(model4)\n",
    "DLhistory.append(history4)\n",
    "AUCVal.append(AUC4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9b701",
   "metadata": {},
   "source": [
    "## Deep Learning Model 5: BiDirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simple bidirectional LSTM with glove embeddings and one dense layer\n",
    "    model5 = Sequential()\n",
    "    model5.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "    model5.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "    model5.add(Dense(1,activation='sigmoid'))\n",
    "    model5.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "history5=model5.fit(X_train_pad, y_train, nb_epoch=5, batch_size=64*strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred5 = model5.predict(xvalid_pad)\n",
    "AUC5 = metrics.roc_auc_score(y_test,y_pred5)\n",
    "print(\"AUC: {:.3f}\".format(AUC5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35738b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'BiderctionalRNN','AUC_Score': AUC5})\n",
    "model1.save('BiRNN2Model.h5')\n",
    "hist_df5 = pd.DataFrame(history5.history)\n",
    "hist_df5.to_csv('History5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DLmodels.append(model5)\n",
    "DLhistory.append(history5)\n",
    "AUCVal.append(AUC5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b570e1c",
   "metadata": {},
   "source": [
    "## Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8496e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Results obtained from various Deep learning models\n",
    "results = pd.DataFrame(scores_model).sort_values(by='AUC_Score',ascending=False)\n",
    "results.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada7b40",
   "metadata": {},
   "source": [
    "## Best Model Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdd1ca",
   "metadata": {},
   "source": [
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f826ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(history4.epoch[-1]+1),history4.history['val_auc'],label='val_auc')\n",
    "plt.plot(range(history4.epoch[-1]+1),history4.history['auc'],label='auc')\n",
    "plt.title('auc'); plt.xlabel('Epoch'); plt.ylabel('auc');plt.legend(); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9587d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(history4.epoch[-1]+1),history4.history['val_loss'],label='Val_loss')\n",
    "plt.plot(range(history4.epoch[-1]+1),history4.history['loss'],label='loss')\n",
    "plt.title('loss'); plt.xlabel('Epoch'); plt.ylabel('loss');plt.legend(); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa082c",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(y_test, y_score):\n",
    "    # print(y_score)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "generate_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa98661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
